#include <pybind11/pybind11.h>
#include <torch/extension.h>

#include "23_flash_attention_infer/fai_kernel.cpp"
#include "23_flash_attention_infer/fai_tilingdata.h"
#include "torch_npu/csrc/core/npu/NPUStream.h"
#include "acl/acl.h"
#include "runtime/rt_ffts.h"
#include "kernel_common.hpp"
#include "kernel_operator.h"

uint32_t GetQNBlockTile(uint32_t qSeqlen, uint32_t groupSize)
{
    uint32_t qRowNumCeil = Q_TILE_CEIL;
    uint32_t qNBlockTile = (qSeqlen != 0) ?
        (qRowNumCeil / qSeqlen) / N_SPLIT_HELPER * N_SPLIT_HELPER : Q_TILE_CEIL;
    qNBlockTile = std::min(qNBlockTile, groupSize);
    qNBlockTile = std::max(qNBlockTile, static_cast<uint32_t>(1));
    return qNBlockTile;
}

uint32_t GetQSBlockTile(int64_t kvSeqlen)
{
    uint32_t qSBlockTile = Q_TILE_CEIL;
    return qSBlockTile;
}

std::vector<at::Tensor>
mha_fwd_kvcache(at::Tensor &q,                 // batch_size x seqlen_q x num_heads x head_size
                const at::Tensor &kcache,            // batch_size_c x seqlen_k x num_heads_k x head_size or num_blocks x page_block_size x num_heads_k x head_size if there's a block_table.
                const at::Tensor &vcache,            // batch_size_c x seqlen_k x num_heads_k x head_size or num_blocks x page_block_size x num_heads_k x head_size if there's a block_table.
                std::optional<const at::Tensor> &k_, // batch_size x seqlen_knew x num_heads_k x head_size
                std::optional<const at::Tensor> &v_, // batch_size x seqlen_knew x num_heads_k x head_size
                std::optional<const at::Tensor> &seqlens_k_, // batch_size
                std::optional<const at::Tensor> &rotary_cos_, // seqlen_ro x (rotary_dim / 2)
                std::optional<const at::Tensor> &rotary_sin_, // seqlen_ro x (rotary_dim / 2)
                std::optional<const at::Tensor> &cache_batch_idx_, // indices to index into the KV cache
                std::optional<const at::Tensor> &leftpad_k_, // batch_size
                std::optional<at::Tensor> &block_table_, // batch_size x max_num_blocks_per_seq
                std::optional<at::Tensor> &alibi_slopes_, // num_heads or batch_size x num_heads
                std::optional<at::Tensor> &out_,             // batch_size x seqlen_q x num_heads x head_size
                const float softmax_scale,
                bool is_causal,
                int window_size_left,
                int window_size_right,
                const float softcap,
                bool is_rotary_interleaved,   // if true, rotary combines indices 0 & 1, else indices 0 & rotary_dim / 2
                int num_splits
                )
{
    auto aclStream = c10_npu::getCurrentNPUStream().stream(false);
    at::Tensor tiling_cpu_tensor = at::empty({1024}, at::device(c10::kCPU).dtype(at::kByte));

    FAInferTilingData* tiling_cpu_ptr = reinterpret_cast<FAInferTilingData*>(tiling_cpu_tensor.data_ptr<uint8_t>());
    uint32_t blockDim = 20;
    at::Tensor seqlens_k, block_table, cache_batch_idx, out;
    // const bool paged_KV = block_table_.has_value();
    const bool paged_KV = false;
    if (seqlens_k_.has_value()) {
        seqlens_k = seqlens_k_.value();
    }
    if (cache_batch_idx_.has_value()) {
        cache_batch_idx = cache_batch_idx_.value();
    }

    if (out_.has_value()) {
        out = out_.value();
    }  else {
        out = torch::empty_like(q);
    }

    if (paged_KV) {
        block_table = block_table_.value();
    }
     //tiling compute
    const auto sizes = q.sizes();
    const int batch_size = sizes[0];
    std::cout << "batch_size" << batch_size << std::endl;
    int seqlen_q = sizes[1];
    std::cout << "seqlen_q" << seqlen_q << std::endl;
    int num_heads = sizes[2];
    std::cout << "num_heads" << num_heads << std::endl;
    const int head_size_og = sizes[3];
    std::cout << "head_size_og" << head_size_og << std::endl;
    const int max_num_blocks_per_seq = !paged_KV ? 0 : block_table.size(1);
    std::cout << "max_num_blocks_per_seq" << max_num_blocks_per_seq << std::endl;
    const int num_blocks = !paged_KV ? 0 : kcache.size(0);
    std::cout << "num_blocks" << num_blocks << std::endl;
    const int page_block_size = !paged_KV ? 128 : kcache.size(1);
    std::cout << "page_block_size" << page_block_size << std::endl;
    const int num_heads_k = kcache.size(2);
    std::cout << "num_heads_k" << num_heads_k << std::endl;
    int64_t* seqlens_k_cpu = static_cast<int64_t *>(seqlens_k.data_ptr());
    std::cout << "1111111111" << std::endl;
    tiling_cpu_ptr->set_batch(static_cast<uint32_t>(batch_size));
    std::cout << "get_batch" << tiling_cpu_ptr->get_batch() << std::endl;
    tiling_cpu_ptr->set_numHeads(static_cast<uint32_t>(num_heads));
    std::cout << "get_numHeads" << tiling_cpu_ptr->get_numHeads() << std::endl;
    tiling_cpu_ptr->set_kvHeads(static_cast<uint32_t>(num_heads_k));
    std::cout << "get_kvHeads" << tiling_cpu_ptr->get_kvHeads() << std::endl;
    tiling_cpu_ptr->set_embeddingSize(static_cast<uint32_t>(head_size_og));
    std::cout << "get_embeddingSize" << tiling_cpu_ptr->get_embeddingSize() << std::endl;
    tiling_cpu_ptr->set_embeddingSizeV(static_cast<uint32_t>(head_size_og));
    std::cout << "get_embeddingSizeV" << tiling_cpu_ptr->get_embeddingSizeV() << std::endl;
    tiling_cpu_ptr->set_numBlocks(static_cast<uint32_t>(num_blocks));
    std::cout << "get_numBlocks" << tiling_cpu_ptr->get_numBlocks() << std::endl;
    tiling_cpu_ptr->set_blockSize(static_cast<uint32_t>(page_block_size));
    std::cout << "get_blockSize" << tiling_cpu_ptr->get_blockSize() << std::endl;
    tiling_cpu_ptr->set_maxNumBlocksPerBatch(static_cast<uint32_t>(page_block_size));
    std::cout << "get_maxNumBlocksPerBatch" << tiling_cpu_ptr->get_maxNumBlocksPerBatch() << std::endl;
    tiling_cpu_ptr->set_maskType(static_cast<uint32_t>(is_causal));
    std::cout << "get_maskType" << tiling_cpu_ptr->get_maskType() << std::endl;
    tiling_cpu_ptr->set_scaleValue(softmax_scale);
    std::cout << "get_scaleValue" << tiling_cpu_ptr->get_scaleValue() << std::endl;
    tiling_cpu_ptr->set_maxQSeqlen(seqlen_q);
    std::cout << "get_maxQSeqlen" << tiling_cpu_ptr->get_maxQSeqlen() << std::endl;
    uint64_t WORKSPACE_BLOCK_SIZE_DB = 128 * 512;
    uint64_t PRELANCH_NUM = 3;
    uint64_t mm1OutSize = static_cast<uint64_t>(blockDim) * WORKSPACE_BLOCK_SIZE_DB *
        4 * PRELANCH_NUM;
    uint64_t smOnlineOutSize = static_cast<uint64_t>(blockDim) * WORKSPACE_BLOCK_SIZE_DB *
        2 * PRELANCH_NUM;
    uint64_t mm2OutSize = static_cast<uint64_t>(blockDim) * WORKSPACE_BLOCK_SIZE_DB *
        4 * PRELANCH_NUM;
    uint64_t UpdateSize = static_cast<uint64_t>(blockDim) * WORKSPACE_BLOCK_SIZE_DB *
        4 * PRELANCH_NUM;
    int64_t workSpaceSize = mm1OutSize + smOnlineOutSize + mm2OutSize + UpdateSize;

    
    at::Tensor workspace_tensor = at::empty({workSpaceSize}, at::device(at::kPrivateUse1).dtype(at::kByte));

    tiling_cpu_ptr->set_mm1OutSize(mm1OutSize);
    tiling_cpu_ptr->set_smOnlineOutSize(smOnlineOutSize);
    tiling_cpu_ptr->set_mm2OutSize(mm2OutSize);
    tiling_cpu_ptr->set_UpdateSize(UpdateSize);
    tiling_cpu_ptr->set_workSpaceSize(workSpaceSize);
    std::cout << "get_mm1OutSize" << tiling_cpu_ptr->get_mm1OutSize() << std::endl;
    std::cout << "get_smOnlineOutSize" << tiling_cpu_ptr->get_smOnlineOutSize() << std::endl;
    std::cout << "get_mm2OutSize" << tiling_cpu_ptr->get_mm2OutSize() << std::endl;
    std::cout << "get_UpdateSize" << tiling_cpu_ptr->get_UpdateSize() << std::endl;
    std::cout << "get_workSpaceSize" << tiling_cpu_ptr->get_workSpaceSize() << std::endl;

    uint32_t totalTaskNum = 0;
    uint32_t groupSize = num_heads / num_heads_k;
    for (int32_t batchIdx = 0; batchIdx < batch_size; batchIdx++) {
        uint64_t qSeqlen = seqlen_q;
        uint64_t kvSeqlen = *(seqlens_k_cpu + batchIdx);
        std::cout << "kvSeqlen" << kvSeqlen << std::endl;
        uint64_t curQNBlockTile = GetQNBlockTile(qSeqlen, groupSize);
        uint64_t qNBlockNumPerGroup = (groupSize + curQNBlockTile - 1) / curQNBlockTile;
        uint64_t curQNBlockNum = qNBlockNumPerGroup * num_heads_k;
        uint64_t curQSBlockTile = GetQSBlockTile(kvSeqlen);
        uint64_t curQSBlockNum = (qSeqlen + curQSBlockTile - 1) / curQSBlockTile;
        uint64_t curTaskNum = curQNBlockNum * curQSBlockNum;
        if (batchIdx == 0) {
            tiling_cpu_ptr->set_firstBatchTaskNum(curTaskNum);
        }
        totalTaskNum += curTaskNum;
    }
    tiling_cpu_ptr->set_totalTaskNum(totalTaskNum);
    std::cout << "get_firstBatchTaskNum" << tiling_cpu_ptr->get_firstBatchTaskNum() << std::endl;
    std::cout << "get_totalTaskNum" << tiling_cpu_ptr->get_totalTaskNum() << std::endl;

    at::Tensor tiling_gpu_tensor = tiling_cpu_tensor.to(at::Device(at::kPrivateUse1));
    at::Tensor seqlenk_gpu_tensor = seqlens_k.to(at::Device(at::kPrivateUse1));
    std::cout << "1111111111" << num_heads_k << std::endl;
    uint64_t fftsAddr{0};
    uint32_t fftsLen{0};
    rtError_t error = rtGetC2cCtrlAddr(&fftsAddr, &fftsLen);
    std::cout << "error" << error << std::endl;
    auto qDevice = static_cast<uint8_t *>(const_cast<void *>(q.storage().data()));
    std::cout << "1111111111" << num_heads_k << std::endl;
    auto kDevice = static_cast<uint8_t *>(const_cast<void *>(kcache.storage().data()));
    std::cout << "1111111111" << num_heads_k << std::endl;
    auto vDevice = static_cast<uint8_t *>(const_cast<void *>(vcache.storage().data()));
    std::cout << "1111111111" << num_heads_k << std::endl;
    auto maskDevice = static_cast<uint8_t *>(const_cast<void *>(vcache.storage().data()));
    std::cout << "1111111111" << num_heads_k << std::endl;
    // auto blockTableDevice = static_cast<uint8_t *>(const_cast<void *>(cache_batch_idx.storage().data()));
    auto blockTableDevice = nullptr;
    std::cout << "1111111111" << num_heads_k << std::endl;
    auto oDevice = static_cast<uint8_t *>(const_cast<void *>(out.storage().data()));
    std::cout << "1111111111" << num_heads_k << std::endl;
    auto qSeqDevice = static_cast<uint8_t *>(const_cast<void *>(seqlenk_gpu_tensor.storage().data()));
    std::cout << "1111111111" << num_heads_k << std::endl;
    auto kvSeqDevice = static_cast<uint8_t *>(const_cast<void *>(seqlenk_gpu_tensor.storage().data()));
    std::cout << "1111111111" << num_heads_k << std::endl;
    auto workspaceDevice = static_cast<uint8_t *>(const_cast<void *>(workspace_tensor.storage().data()));
    std::cout << "1111111111" << num_heads_k << std::endl;
    auto tilingDevice = static_cast<uint8_t *>(const_cast<void *>(tiling_gpu_tensor.storage().data()));
    std::cout << "1111111111" << num_heads_k << std::endl;
    SplitFuse::FAInfer<half, half, float, false, FaiKenel::MaskType::NO_MASK, FaiKenel::inputLayout::BSND><<<blockDim, nullptr, aclStream>>>(
                fftsAddr, qDevice, kDevice, vDevice, maskDevice, blockTableDevice, oDevice, nullptr,
                qSeqDevice, kvSeqDevice, workspaceDevice, tilingDevice);
    return {out, out};
}

// std::vector<at::Tensor>
// mha_fwd_kvcache(at::Tensor &q,                                    
//                 at::Tensor &k,                        
//                 at::Tensor &v,                         
//                 at::Tensor &mask,               
//                 at::Tensor &blockTables,            
//                 at::Tensor &o,       
//                 at::Tensor &actualQseqlen,      
//                 at::Tensor &actualKvseqlen,      
//                 at::Tensor &s, 
//                 at::Tensor &p,      
//                 at::Tensor &oTemp,           
//                 at::Tensor &oUpdate,          
//                 at::Tensor &tiling,                   
//                 uint64_t fftsAddr)
// {
//     auto aclStream = c10_npu::getCurrentNPUStream().stream(false);
//     at::Tensor cpu_struct_tensor = at::empty({1024}, at::device(c10::kCPU).dtype(at::kByte));
//     at::Tensor workspace_tensor = at::empty({512*512*4}, at::device(at::kPrivateUse1).dtype(at::kByte));
    
//     MyFeature* struct_cpu_ptr = reinterpret_cast<MyFeature*>(cpu_struct_tensor.data_ptr<uint8_t>());
//     struct_cpu_ptr->x = 1.10;
//     at::Tensor gpu_struct_tensor = cpu_struct_tensor.to(at::Device(at::kPrivateUse1));
//     uint32_t blockDim = 20;
//     auto qGm = static_cast<uint8_t *>(const_cast<void *>(q.storage().data()));
//     auto kGm = static_cast<uint8_t *>(const_cast<void *>(k.storage().data()));
//     auto vGm = static_cast<uint8_t *>(const_cast<void *>(v.storage().data()));
//     auto maskGm = static_cast<uint8_t *>(const_cast<void *>(mask.storage().data()));
//     auto blockTablesGm = static_cast<uint8_t *>(const_cast<void *>(blockTables.storage().data()));
//     auto oGm = static_cast<uint8_t *>(const_cast<void *>(o.storage().data()));
//     auto actualQseqlenGm = static_cast<uint8_t *>(const_cast<void *>(actualQseqlen.storage().data()));
//     auto actualKvseqlenGm = static_cast<uint8_t *>(const_cast<void *>(actualKvseqlen.storage().data()));
//     auto sGm = static_cast<uint8_t *>(const_cast<void *>(workspace_tensor.storage().data()));
//     auto pGm = static_cast<uint8_t *>(const_cast<void *>(workspace_tensor.storage().data()));
//     auto oTempGm = static_cast<uint8_t *>(const_cast<void *>(workspace_tensor.storage().data()));
//     auto oUpdateGm = static_cast<uint8_t *>(const_cast<void *>(workspace_tensor.storage().data()));
//     auto tilingGm = static_cast<uint8_t *>(const_cast<void *>(gpu_struct_tensor.storage().data()));
//     FAInferBf16<<<blockDim, nullptr, aclStream>>>(
//                 fftsAddr, qGm, kGm, vGm, maskGm, blockTablesGm, oGm, actualQseqlenGm, actualKvseqlenGm,
//                 sGm, pGm, oTempGm, oUpdateGm, tilingGm
//             );
//     return {oUpdate, oTemp};
// }

PYBIND11_MODULE(flash_attn_2_cuda, m)
{
    m.doc() = "FlashAttention"; // optional module docstring
    m.def("fwd_kvcache", &mha_fwd_kvcache, "Forward pass, with KV-cache");
}